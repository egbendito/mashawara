{
    "docs-architecture-html": {
        "title": "Data Architecture",
        "content": "Data Architecture Each instance of the Mashawara DST starts from a similar structure of the root directory. This space has a specific architecture to facilitate the management and use of the tool, and users are encouraged to maintain it or keep it to the extent possible. This design facilitates the separation between data and processes. Figure 4 describes the architecture of the system. It is important to observe the clear separation into 2 main sections: data and functions. The folder “data” is the section where the information should be stored, and it is described with detail in 1. The folder “functions” contains the scripts and processes running in the DST and it is described in 2. Figure 2: Architecture of the SAA DST, containing all the necessary components. 1 Inputs There are data input requirements for the DST to execute, such as environmental information, but also inputs required by DSSAT software, or others that need to be provided by the user (such as the locations to simulate). This data is stored under ./data/inputs. Again, this has three sub-folders with different characteristics. 1.1 Main The first sub-folder is main, and it is mainly referring to spatial or environmental datasets typically stored as NetCDF or GeoTIFF files. This directory should be updated using the specific functions to download and source data (see 6.1 Download data). At this moment main contains 3 sub-folders: soil, weather and administrative, with a README.md describing the contents. Each of these folders contains available databases. Figure 2 indicates the substructure for this folder. Unless the user wants to add a specific environmental database, it is better not to add things here. Figure 3: main folder under data/inputs containing the spatial and environmental databases. 1.2 DSSAT Another sub-folder is dssat, which stores template format files and input data necessary for DSSAT (figure 7). This includes experimental files (.MZX) with template management design for the simulations. These management templates need to be kept track in order to know if the DST runs on a different scenario, and thus the files should be deposited using the labeling convention vYYYYMMDD.MZX, where: v: Indicates a version YYYY: Indicates the year of the DST execution MM: Indicates the month of the DST execution DD: Indicates the day of the DST execution .MZX: File extension for DSSAT Figure 4: The dssat folder with its specific structure containing templates (xfiles, NP_ERR) and the SAA.CUL with the specific varieties part of the DST. Cultivar files (vYYYMMDD.CUL) are DSSAT files containing genotype coefficients for different crop varieties. These are calibrated coefficients for specific locations, or generic ones. In the dssat sub-folder there should be a versioned .CUL file, following the same name convention of vYYYYMMDD.CUL which contains the different coefficients for the varieties used in the DST. Users should add new varieties with the relevant variables (“VAR#”, “VRNAME”, “P1”, etc) to this file. The DST will only execute the varieties included in this file. It is also important to consider that the tool will execute all varieties included in this file. @VAR# VRNAME.......... EXPNO ECO# P1 P2 P5 G2 G3 PHINT ! 1 2 3 4 5 6 IF0012 DT STR W . IB0001 302.0 0.400 805.9 780.0 6.50 40.00 IF0014 COMP1 SYN NEW . IB0001 253.3 0.424 794.9 743.3 6.25 38.90 IF0019 EVDT 99 . IB0001 199.5 0.300 789.0 720.4 6.69 40.00 1.3 User The user folder contains data provided by the user. These could be files containing latitude (Y) and longitude (X) coordinates in decimal degrees and WGS84 (EPSG:4326) SRID. The first column should refer to the longitude (X) and the second column to latitude (Y). The files should be stored as a (.CSV) with the same labeling convention as the experimental files (.MZX). 2 Intermediate Data produced in the DSSAT component of the DST (component B, figure 1) are stored under ./data/intermediate and this only includes one single sub-flder refering to the different DSSAT executions of the DST and are labeled with the versioning indicated in 1. These are auto-generated sub-directories upon execution of the DST. These folders contain a series of directories labeled as EX0000, referring to each location (pair of longitude and latitude) for the simulation. Each of these represents a simulated location and contains those DSSAT files in the specific formats of the software, which include .WTH, .SOL, .MZX and .OUT. They are generated by the dssat functions (see DSSAT under Functions) and the user does not need to interact with them. The “.OUT” (output files) comprise the crop growth database (figure 1) and are the inputs for the aggregation functions. 3 Outputs The final outputs of the DST are aggregated results from the DSSAT .OUT crop growth database. These are also auto-generated files following the mentioned naming convention and are stored in .CSV tabular format under ./data/outputs. The files contain the final and necessary information for the recommendations: lookup_key: Location unique identifier var: Variety recommended pdate: Planting date to be recommended rank: for each location the ranking of each variety and planting date recommendation.",
        "url": " /docs/architecture.html",
        "author": ""
    },
    "docs-functions-html": {
        "title": "Functions",
        "content": "Functions 1 Download data Necessary data for the execution of the DST has to be pre-populated into the system. This process is carried by the tools stored in ./functions/01_download. These set of functions do not run with the rest of the DST, since these are heavy processes which take time. Instead, the data manager will need to make sure the data is updated and execute the functions as per the update schedules, or in worst case after data losses. The data sources rely on CG Labs internal data vault to access and repopulate the data inputs for environmental data. Each download process is specific for the different data sources in ./data/input/main and needs to be executed separately from the CG Labs terminal. For example, to download the data for chirps data, the user would need to execute the following command: jovyan@user:~$ cd /home/jovyan/saa-use-case/functions/01_download jovyan@user:~/saa-use-case/functions/01_download$ Rscript 01_2_download_chirps.R Where 2022 is the year that needs to be updated. It is important that the user executes the script from the local script directory (./functions/01_download), as shown in the example above. The outputs will be automatically stored in the relevant folder in ./data/input/main 2 Extract, Transform and Load The functions stored in ./functions/02_etl serve to interact with the relevant data inputs in ./data/input/main. They are a typical set of ETL (Extract, Transform and Load) procedures that allow to provide data for the DST, and particularly for the DSSAT software. These are stand-alone procedures integrated into the DSSAT routines, so the user does not need to execute them, unless a new variable from the dataset is implemented, or a new dataset is being added, in which case an ETL will need to be developed. At the moment there are 4 ETL processes providing inputs in the DST: 02_01_isda.R for soil variables; 02_02_agera5.R for climatic variables (except rainfall); 02_03_chirps.R for rainfall; and 02_04_gps.R to provide GPS coordinates in the target area if the user does not provide specific ones. These are stand-alone procedures integrated into the DSSAT routines, so the user does not need to execute them, but below are examples of how they work: get.isda(X = 9.578, Y = 10.564) Which would return something like: iso X Y depth lyr_center clay sand silt bulk_density ph 1 NG 9.578 10.564 20 10 21 55 23 1.36 6.1 2 NG 9.578 10.564 50 35 27 52 23 1.35 6.2 For extracting the data from CHIRPS, for example: chirps(startDate = \"2020-07-01\", endDate = \"2020-07-13\", coordPoints = data.frame(X = 9.578, Y = 10.564)) Returning: X Y dates year month day rain 1 9.578 10.564 2020-07-01 2020 07 01 0.000000 2 9.578 10.564 2020-07-02 2020 07 02 0.000000 3 9.578 10.564 2020-07-03 2020 07 03 20.258272 4 9.578 10.564 2020-07-04 2020 07 04 20.258272 5 9.578 10.564 2020-07-05 2020 07 05 20.258272 6 9.578 10.564 2020-07-06 2020 07 06 5.997376 7 9.578 10.564 2020-07-07 2020 07 07 11.994753 8 9.578 10.564 2020-07-08 2020 07 08 5.997376 9 9.578 10.564 2020-07-09 2020 07 09 5.997376 10 9.578 10.564 2020-07-10 2020 07 10 11.994753 11 9.578 10.564 2020-07-11 2020 07 11 0.000000 12 9.578 10.564 2020-07-12 2020 07 12 33.512047 13 9.578 10.564 2020-07-13 2020 07 13 6.702409 3 DSSAT As mentioned, this is the engine of the DST. These functions are the processes that form component B shown in figure 1, and are stored in ./functions/03_dssat. Just as the other download and ETL processes, the user does not need to interact with these scripts in order to execute the DST. They automatically generate and run the necessary steps to run and execute DSSAT, and depend on 02_etl (see 6.2 Extract, Transform and Load) to prepare the necessary soil and weather data requirements. The functions provide the possibility to be executed in parallel for computationally intensive runs of the DST, such as multiple years, over large number of locations (e.g.; 100). They require certain inputs to be provided by the user, such as the target area or locations to be simulated and the range of dates to cover. The functions automatically read and construct the necessary folder structure to store the intermediate DSSAT results under ./data/intermediate/dssat. The processes are sequential and need to be executed in order, but this is handled by the DST. Below are examples of these functions. The function 03_01_dssat_inputs.R is responsible for preparing the necessary environmental data (soil and weather) in the required DSSAT formats. It uses the set of locations and range of times to extract the relevant information as per below: dssat.extdata(coords = data.frame(\"longitude\" = c(9.578), \"latitude\" = c(10.564)), sdate = \"2023-01-01\", edate = \"2023-12-31\", jobs = 1, path.to.ex = “data/intermediate/dssat/v20231231”) The code above will extract and write DSSAT .WTH and .SOL files to a folder ./data/intermediate/dssat/v20231231/EXTE0000/ for the range of dates indicated and the coordinates provided. The DST is designed to simulate the entire year (1st Jan to 31st Dec), so the .WTH files will contain daily weather observations for each entire year in the date range. DSSAT requires an Xfile or experimental file which defines the scenario to be simulated. This file is generated using the 03_02_dssat_experiment.R script and requires the same arguments to be defined: dssat.Xdata(coords = data.frame(\"longitude\" = c(9.578), \"latitude\" = c(10.564)), sdate = \"2023-01-01\", edate = \"2023-12-31\", jobs = 1, path.to.ex = “data/intermediate/dssat/v20231231”) This produces the .MZX file using the template vYYYYMMDD.MZX (see Inputs) provided by the user in ./data/inputs/dssat/xfiles. These files look something like: *EXP.DETAILS: *GENERAL @PEOPLE @ADDRESS @SITE X=7.82500000069449, Y=12.2749999676097 *TREATMENTS -------------FACTOR LEVELS------------ @N R O C TNAME.................... CU FL SA IC MP MI MF MR MC MT ME MH SM 1 1 1 0 IF0012_PD_1 1 1 0 1 1 0 1 0 0 0 0 0 1 2 1 1 0 IF0012_PD_2 1 1 0 1 2 0 1 0 0 0 0 0 1 *CULTIVARS @C CR INGENO CNAME 1 MZ IF0012 DT STR W *FIELDS @L ID_FIELD WSTA.... FLSA FLOB FLDT FLDD FLDS FLST SLTX SLDP ID_SOIL 1 00000001 WHTE0081 -99 -99 -99 -99 -99 -99 SIL 237 ISDA000081 Once the required files (.SOL, .WTH and .MZX) are generated, it is possible to execute DSSAT. The function in 03_03_dssat_execute.R uses the same inputs as the other two dssat functions shown above, and launches a batch execution. The DSSAT output files are stored in ./data/intermediate/dssat/v20231231/EXTE0000/, but only the .OUT files are kept. Below is an example of how DSSAT is executed in the DST: dssat.execute(coords = data.frame(\"longitude\" = c(9.578), \"latitude\" = c(10.564)), sdate = \"2023-01-01\", edate = \"2023-12-31\", jobs = 1, path.to.ex = “data/intermediate/dssat/v20231231”) 4 Aggregation The final outputs from the DST are generated through the functions in ./functions/04_aggregation. There are two processes in this component (C in figure 1): 04_01_aggregation_dssat.R which aggregates DSSAT outputs; and 04_02_aggregation_rank.R which ranks the aggregated outputs and produces the final look-up table for the DST (Outputs). Both processes are again automated and part of the processing chain, so the user does not need to explicitly run the scripts. The function 04_01_aggregation_dssat.R reads the DSSAT .OUT files for all locations, varieties and planting dates in the DST run, and puts everything into a single temporary table. It is executed as follows: dssat.aggregate(years = c(2020:2023), jobs = 1, path.to.ex = “data/intermediate/dssat/v20231231”) The final ranked outputs from the DST are generated through the script 04_02_aggregation_rank.R, which produces several metrics of the yield (upper limit, mean, lower limit and coefficient of variation) to determine the most appropriate combination of variety and planting windows in each simulated location. The function in this script can be executed as follows: rank.aggregate(years = c(2020:2023), jobs = 1, path.to.ex = “data/intermediate/dssat/v20231231”) Which stores the final DST output in a CSV tabular format in ./data/outputs/ (see Outputs). 5 Executing the DST To execute the DST, the user can run a command from the terminal. jovyan@user:~$ cd /path/to/mashawara/functions jovyan@user:/path/to/mashawara/functions$ Rscript 0_saa.R Kano 2022-01-01 2022-12-31 6 It is recommended to execute the DST from the functions directory, as shown in the first line of the block above. In the example, the Rscript command is used to execute R scripts from the terminal, in this case, we are executing the 0_saa.R script, a ‘mother’ script containing all other scripts and putting the different processes together. After this the user needs to provide a set of 4 arguments. The first argument is the set of GPS coordinates corresponding to each point simulation (in the example above Kano). Optionally, the user can provide a table in CSV format with the file name convention (as indicated in User) and storing it under the relevant user folder. The 2 next arguments are the start and end dates (2022-01-01 and 2022-12-31 in the example), composing the date ranges that the DST will be executed for. This can span over multiple years, and at least, the DST will execute an entire calendar year, even if the year is the same both at the start and end dates. Finally, the user needs to provide the number of parallel processes to be executed (6 in the example). This process also checks that the input requirements are met, including the .MZX file with the appropriate name, and will write the intermediate and final outputs in the relevant folders.",
        "url": " /docs/functions.html",
        "author": ""
    },
    "": {
        "title": "Mashawara",
        "content": "About Mashawara Mashawara stands for “maize advise” in Hausa language of Northern Nigeria. The tool works as stand-alone software aimed to provide agronomic advisory for multiple crops on opimal time of planting and variety selection. The DST is based on DSSAT software as a backend engine, and implemented with a set of R scripts relying on several other R packages (see dependencies). This software is part of Excellence in Agronomy and was initiated with the support of the Sasakawa use case, thus the name. It has been later expanded to other use cases, regions and crops to provide a dynamic tool easily deployable and updated. The tool is free and open to be used by anyone to reuse extend or improve upn under the CC-BY license. Read more about the license in the license section of this documentation.",
        "url": " /",
        "author": ""
    },
    "docs-installation-html": {
        "title": "Installation",
        "content": "Installation The tool is designed to be deployed in a CG Labs like environment under its current deployment in the CGIAR system. In subsequent updates the tool will try to rely on cloud available data to make it system agnostic. 1. How to install? To create an instance of the DST, the user should clone the GitHub repository and execute the following code: This is a sample block of code Here the ./setup.sh should go. Look at github repo 2. Configuration Users can compile their own version by cloning the repo and running remotes::install_github(\"reagro/carobiner\") ff &lt;- carobiner::make_carob(path) where path is the folder of the cloned repo (e.g. “d:/github/carob”) You can also compile your own version of the full carob by clicking below Compile Carob Or compile the CC-BY data below Compile Carob CC-BY 3. How to upgrade In future releases and updates of the tool the user will need to re-start from scratch… But better not do it like that…",
        "url": " /docs/installation.html",
        "author": ""
    },
    "docs-overview-html": {
        "title": "Overview",
        "content": "Overview Seasonal variability is a major constraint for many farmers under rainfed conditions in many parts of Africa. Climate change is resulting in more erratic seasonal weather patterns and limiting the ability of farmers to rely on previous knowledge of the farming system where they operate. The Mashawara (Variety and Planting Window Recommendation) Decision Support Tool (DST) for EiA aims to provide farmers with spatially explicit recommendations on optimal variety and planting window combinations. The recommendations are based on DSSAT v47 using a spatialization framework and consuming data from iSDA soil, CHIRPSv2 and ECMWF AgERA5. Additionally, calibrated cultivar coefficients are used to reproduce the local behavior of cultivar varieties in the target area. The tool is expected to be updated every cropping season (year), to account for the previous year’s conditions on the recommendations. In general terms, the DST is composed of 3 main components: inputs, DSSAT v48 and aggregations. Figure 1 presents the general overview with the 3 main components as well as the different elements and categories in the system. These 3 components are sequential, and must be executed in the appropriate order (A, B, C) to produce the expected results. Component A refers to the data input requirements to execute the process, B is related to the formatting, set-up and execution of DSSAT and C generates the final outputs with the recommendations. In this DST, external as well as internal data is used along the process, and only essential data will be persisted in the system. The entire process is publicly accessible on GitHub for partners or anyone else. Figure 1: General overview of the variety and planting window advice DST. Component A refers to the external data sources for the environmental data. Component B contains DSSAT related processes. Component C is the final step dealing with the aggregation and formatting of the final data outputs. In the context of this DST, variety and planting windows are the focus model parameters, and are defined as the values (variety and planting dates) which maximize the yield, since this is the most important parameter for farmers and the one which determines most agronomic decisions. This tool therefore provides the final results as a combination of optimal variety and planting times which maximize the yield.",
        "url": " /docs/overview.html",
        "author": ""
    },
    ".": {
        ".": "."
    }
}
